{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Convert to BOW vectors\n",
    "A = np.random.randint(0,100, size=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Apply SVD and pick r = 9\n",
    "U, S, VT = np.linalg.svd(A)\n",
    "r = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Compute embedding for all 15 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "VT_row = VT[:r, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute embeddings for each document (as A*V)\n",
    "row_embeddings = np.matmul(A, VT_row.T)\n",
    "row_embeddings_dict = dict()\n",
    "for i, embedding in enumerate(row_embeddings):\n",
    "    row_embeddings_dict[\"doc\" + str(i + 1)] = list(map(round, list(embedding), [2] * r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc1': [-201.49, 35.3, -38.34, 19.77, 8.59, 18.24, 18.29, 22.89, 19.29],\n",
       " 'doc2': [-147.26, -70.19, -26.35, 5.38, 54.18, -2.28, 2.21, 10.97, -8.43],\n",
       " 'doc3': [-145.09, 26.81, 38.77, 33.8, 3.94, -36.21, 22.54, -44.29, -4.61],\n",
       " 'doc4': [-190.2, -43.32, 13.14, -15.7, -39.66, 1.77, 14.61, 24.71, -5.47],\n",
       " 'doc5': [-132.83, -19.84, 57.46, -16.56, 0.53, 5.76, 29.67, 11.23, -2.67],\n",
       " 'doc6': [-122.42, 87.1, -0.45, -64.67, 6.6, -33.02, -25.73, 17.98, -19.92],\n",
       " 'doc7': [-176.57, -24.33, 26.83, -34.24, 30.45, 24.33, 2.54, -13.11, -4.95],\n",
       " 'doc8': [-111.91, 10.42, -52.93, -1.56, -5.22, 57.57, -19.43, -36.29, -13.46],\n",
       " 'doc9': [-181.56, -71.17, -15.71, -27.35, -43.39, -3.15, -3.69, -6.77, -5.0],\n",
       " 'doc10': [-206.53, -10.93, 9.4, -33.49, 3.95, -30.78, -17.87, -25.97, 27.13],\n",
       " 'doc11': [-169.18, 31.94, 19.64, 27.51, 42.18, 8.29, 5.42, 8.96, -6.43],\n",
       " 'doc12': [-124.33, -37.22, -60.29, 61.51, -2.53, -43.56, -21.63, 7.94, -8.48],\n",
       " 'doc13': [-105.75, 28.98, -54.54, -22.75, 4.85, 6.57, -4.21, 6.63, 18.69],\n",
       " 'doc14': [-185.75, 71.43, -19.03, 29.17, -37.05, 6.43, 22.3, -1.2, -6.0],\n",
       " 'doc15': [-134.84, 5.53, 85.12, 46.08, -13.35, 23.5, -49.63, 13.29, 6.1]}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_embeddings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Compute embedding for all 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_col = U[:, :r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute embeddings for each word (as U^T*A)\n",
    "column_embeddings = np.matmul(U_col.T, A).T\n",
    "column_embeddings_dict = dict()\n",
    "for i, embedding in enumerate(column_embeddings):\n",
    "    column_embeddings_dict[\"word\" + str(i + 1)] = list(map(round, list(embedding), [2] * r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word1': [-198.48, -72.47, 0.17, -54.0, 2.16, 42.5, 4.41, 10.14, 0.93],\n",
       " 'word2': [-229.25, -36.06, 4.4, 61.75, 52.02, -24.49, 5.65, 22.33, 22.12],\n",
       " 'word3': [-169.56, 25.05, 98.28, -32.86, 19.21, -24.56, -48.48, -5.29, -7.1],\n",
       " 'word4': [-184.76, 15.79, 25.26, -39.47, -48.4, 1.3, 12.88, 40.16, 15.81],\n",
       " 'word5': [-169.09, 4.15, -32.55, 67.41, -59.41, 3.88, -38.9, 6.32, -4.37],\n",
       " 'word6': [-173.17, -72.72, 4.26, -9.66, -31.15, -61.04, 26.8, -32.39, -6.43],\n",
       " 'word7': [-169.51, 110.36, -45.99, -28.28, 0.71, -10.0, 4.62, -29.09, 21.04],\n",
       " 'word8': [-240.77,\n",
       "  -30.04,\n",
       "  -89.27,\n",
       "  -25.31,\n",
       "  26.59,\n",
       "  13.63,\n",
       "  -23.31,\n",
       "  -9.46,\n",
       "  -9.86],\n",
       " 'word9': [-224.5, 18.01, 62.67, 43.26, -1.27, 54.23, 24.96, -31.9, -4.28],\n",
       " 'word10': [-169.95, 69.79, -14.04, 5.15, 14.9, -16.15, 28.44, 30.92, -31.49]}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_embeddings_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
