{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTwDFnjHMk25"
      },
      "source": [
        "# CS246 - Homework 1\n",
        "## Question 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXQzA01OS_yQ"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbYZoVVWOZA5"
      },
      "source": [
        "Let's setup Spark on your Colab environment.  Run the cell below!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhzk3GE6S9RC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93dd08f9-44e8-40b6-e162-cfce0e3fa906"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive2\n",
        "#the output 'xxx is not a symbolic link' will not affect your implementation or execution\n",
        "#to fix 'xxx is not a symbolic link', you can uncomment the lines starting from !mv xxxx\n",
        "#you may need to replace xxx.11 with the correct version if other errors come up after colab update\n",
        "#to get the correct version, use !ls /usr/local/lib to find out\n",
        "!mv /usr/local/lib/libtbbmalloc_proxy.so.2 /usr/local/lib/libtbbmalloc_proxy.so.2.backup\n",
        "!mv /usr/local/lib/libtbbmalloc.so.2 /usr/local/lib/libtbbmalloc.so.2.backup\n",
        "!mv /usr/local/lib/libtbbbind_2_5.so.3 /usr/local/lib/libtbbbind_2_5.so.3.backup\n",
        "!mv /usr/local/lib/libtbb.so.12 /usr/local/lib/libtbb.so.12.backup\n",
        "!mv /usr/local/lib/libtbbbind_2_0.so.3 /usr/local/lib/libtbbbind_2_0.so.3.backup\n",
        "!mv /usr/local/lib/libtbbbind.so.3 /usr/local/lib/libtbbbind.so.3.backup\n",
        "!ln -s /usr/local/lib/libtbbmalloc_proxy.so.2.11 /usr/local/lib/libtbbmalloc_proxy.so.2\n",
        "!ln -s /usr/local/lib/libtbbmalloc.so.2.11 /usr/local/lib/libtbbmalloc.so.2\n",
        "!ln -s /usr/local/lib/libtbbbind_2_5.so.3.11 /usr/local/lib/libtbbbind_2_5.so.3\n",
        "!ln -s /usr/local/lib/libtbb.so.12.11 /usr/local/lib/libtbb.so.12\n",
        "!ln -s /usr/local/lib/libtbbbind_2_0.so.3.11 /usr/local/lib/libtbbbind_2_0.so.3\n",
        "!ln -s /usr/local/lib/libtbbbind.so.3.11 /usr/local/lib/libtbbbind.so.3\n",
        "#If error related to the above execution occurs, you can try commenting out the above 12 lines under pip install PyDrive2 (not included)\n",
        "\n",
        "# !sudo ldconfig\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=680c29eceb773bc19795dd91d8086e5b16e528f766fd47e31568f320ef270415\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hThe following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum fonts-ipafont-gothic\n",
            "  fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 39.7 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 121671 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u392-ga-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u392-ga-1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u392-ga-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u392-ga-1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u392-ga-1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u392-ga-1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctU1dYjfOif7"
      },
      "source": [
        "Now we authenticate a Google Drive client to download the file we will be processing in our Spark job.\n",
        "\n",
        "**Make sure to follow the interactive instructions.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dfnX7IAOkvH"
      },
      "source": [
        "from pydrive2.auth import GoogleAuth\n",
        "from pydrive2.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id='1Qaa1YlL_ihEBv_36OECIvK8_pSKWT7Sj'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('soc-LiveJournal1Adj.txt')"
      ],
      "metadata": {
        "id": "DrVS1zQZlizf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgQaRx6rSaf9"
      },
      "source": [
        "# Let's import the libraries we will need\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUD5XpD_SagA"
      },
      "source": [
        "Let's initialize the Spark context.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ft3VivrSagB"
      },
      "source": [
        "# create the Spark Session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# create the Spark Context\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n20ixkgSagD"
      },
      "source": [
        "You can easily check the current version and get the link of the web interface. In the Spark UI, you can monitor the progress of your job and debug the performance bottlenecks (if your Colab is running with a **local runtime**)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl4RHbqFSagE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "58d3068c-af55-40cc-c939-a8f4fc5b6215"
      },
      "source": [
        "spark"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ce017e28d30>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://3f64a806ffce:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gewv-lKMSagI"
      },
      "source": [
        "# Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First read the document."
      ],
      "metadata": {
        "id": "lI2c3AROnMSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt = spark.read.text(\"soc-LiveJournal1Adj.txt\")\n",
        "\n",
        "# Split the txt file data into id and friends columns\n",
        "data = txt.select(split(col(\"value\"), \"\\t\").alias(\"split_row\"))\\\n",
        "          .select(col(\"split_row\")[0].alias(\"id\"), split(col(\"split_row\")[1], \",\").alias(\"friends\"))\\\n",
        "          .select(\"id\", explode(\"friends\").alias(\"friend\"))\n",
        "\n",
        "# Count number of occurances of friends of friends\n",
        "count_data = data.alias(\"d1\").join(data.alias(\"d2\"), \"friend\")\\\n",
        "                 .filter(col(\"d1.id\") != col(\"d2.id\"))\\\n",
        "                 .groupBy(\"d1.id\", \"d2.id\")\\\n",
        "                 .agg(count(\"*\").alias(\"count\"))\n",
        "\n",
        "# Rank number of occurances and keep top 10 for each id\n",
        "spec = Window.partitionBy(\"d1.id\").orderBy(desc(\"count\"), \"d2.id\")\n",
        "top10 = count_data.withColumn(\"rank\", rank().over(spec))\\\n",
        "                  .filter(col(\"rank\") <= 10)\\\n",
        "                  .groupBy(\"d1.id\")\\\n",
        "                  .agg(collect_list(\"d2.id\").alias(\"recommendations\"))\\\n",
        "                  .orderBy(\"d1.id\")"
      ],
      "metadata": {
        "id": "zxRYySbi-CyW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = [11, 924, 8941, 8942, 9019, 9020, 9021, 9022, 9990, 9992, 9993]\n",
        "filtered_results = top10.filter(col(\"d1.id\").isin(ids))\n",
        "filtered_results.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvU3U9GaFehY",
        "outputId": "fa206a74-a746-4eaa-bea5-bbc36d851795"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------------------------------------------------------------------+\n",
            "|id  |recommendations                                                       |\n",
            "+----+----------------------------------------------------------------------+\n",
            "|11  |[27552, 27573, 27574, 27589, 27590, 27600, 27617, 27620, 27667, 32072]|\n",
            "|8941|[8938, 8942, 8946, 8939, 8943, 8944, 8945, 8940]                      |\n",
            "|8942|[8938, 8939, 8941, 8945, 8946, 8940, 8943, 8944]                      |\n",
            "|9019|[320, 9018, 9016, 9017, 9020, 9021, 9022, 317, 9023]                  |\n",
            "|9020|[9021, 320, 9016, 9017, 9018, 9019, 9022, 317, 9023]                  |\n",
            "|9021|[9020, 320, 9016, 9017, 9018, 9019, 9022, 317, 9023]                  |\n",
            "|9022|[9019, 9020, 9021, 317, 320, 9016, 9017, 9018, 9023]                  |\n",
            "|924 |[11860, 15416, 2409, 43748, 439, 45881, 6995]                         |\n",
            "|9990|[9987, 9988, 9989, 9993, 9994, 35667, 9991, 9992, 13134, 13478]       |\n",
            "|9992|[9987, 9989, 35667, 9988, 9990, 9993, 9994, 9991]                     |\n",
            "|9993|[9990, 9994, 9987, 9988, 9989, 9991, 35667, 9992, 13134, 13478]       |\n",
            "+----+----------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}